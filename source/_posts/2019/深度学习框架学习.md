---
title: 深度学习框架学习
tags:
  - 深度学习
  
date: 2019-06-20 20:45:38
abbrlink: 79vml
---

tensorflow
TensorFlow 支持 14 种不同的类型，实数
（ tf.float32 tf.float64 ）、整
数（ tf.int8 tf.intl6 tf.int32 tf.int64 tf.uint8 ）、布尔型 tf.bool) 和复数（ tf.complex64
tf.complex128
计算图
会话
张量 name shape dtype
每个节点都是一个运算，而每 条边代表了计算之间的依赖关系

tf.Graph()
tf.get_default_graph()

变量.initializer

为了方便 变量初始化用tf.global_variables_initializer

##用placeholder代替常量
tf.placeholder(tf.float32,shape=(1,2),name='input')


sess=tf.Session()

y是一堆操作
sess.run(y,feed_dict={x:[[0.8,0.9]]})

##损失函数刻画当前预测值和真实答案之间的差距。然后通过反向传播使差距减小 （优化目标）

tf.train.AdadeltaOptimizer(learning_rate= 学习率).minimize(损失函数)

tf.nn.relu
tf.sigmoid
tf.tanh

均方误差 MSE

tf.where  相似于条件表达式
tf.greater
多层网络解决异或运算

噪声
避免过拟合 正则化 （限制权重大小 ）

滑动平均模型

##训练数据包括训练集和验证集 验证集来‘训练’超参数
tf.variable_scope 命名空间
tf.get_variable  reuse=True 已经创建的变量   reuse=False 新建变量

tensorflow模型持久化 tf.train.Saver()
.meta 计算图的结构   
.ckpt  变量的取值

卷积和池化 特征提取
全连接   分类

##参数计算：
卷积神经网络
过滤器长*宽*深*输入深度+过滤器深

全连接网络
输入*节点个数+节点个数

输出特征图尺寸：
（（输入-过滤器+2*填充）/步长）+1

池化层 减小矩阵尺寸 一般不改变深度 减少最后全连接层的参数

卷积层使用过滤器横跨整个深度，池化层过滤器只影响一个深度的节点

tf.nn.conv2d
tf.nn.max_pool
tf.nn.avg_pool

全连接参数个数：
输入*输出+输出

Inception 将卷积层并联在一起

当使用全0填充 步长为1时 输出矩阵大小相同 将深度拼接

Inception-v3有46层 11个Inception模块组成  有96个卷积层

迁移学习   瓶颈层

\ 一行不够下一行接着写

tensorflow多线程处理输入数据  队列

循环神经网络
序列数据
长期依赖问题   提出LSTM


##tensorBoard可视化高维向量

input1=tf.constant([1.0,2.0,3.0],name="input1")
input2=tf.Variable(tf.random_uniform([3]),name="input2")
output=tf.add_n([input1,input2],name="add")
writer=tf.summary.FileWriter('/home/zhangxuefeng/log',tf.get_default_graph())
writer.close()

tensorboard --logdir={pwd}

name_scope和Variable_scope区别
tf.get_variable不受name_scope影响 不加命名空间

深度学习训练并行模式 多GPU并行  同步模式（效率不如异步模式）  异步模式（参数更新有问题）

分布式Tensorflow
#pytorch
[清华大学开源软件镜像站](https://mirrors.tuna.tsinghua.edu.cn/)

conda config --add channels
##检查pytorch是否支持GPU
import torch
torch.cuda.is_available()
##pyTorch基础
Tensor可以和numpy的ndarray可以相互转换 区别在于Tensor可以在GPU运行，而ndarray只能CPU运行
不同数据类型的Tensor,torch.FloatTensor,torch.DoubleTensor,torch.ShortTensor,torch.IntTensor,torch.LongTensor
print("a is :{}".format(a))
torch.zeros((2,3))
torch.randn((3,2))

torch.utils.data.Dataset

torchvision 数据读取类 ImageFolder

torch.nn.Module
torch.optim 优化
一阶优化算法 梯度下降
二阶优化算法 Hessian方法  基于牛顿法

##模型的保存和加载
torch.save()   第一个参数保存对象  第二个参数是保存路径和名称

nn.Linear()
nn.Sigmoid()
##损失函数
通过梯度*学习率更新参数
sigmoid函数的两个缺点  
（1）梯度消失   
（2）非0均值
tanh函数
仍然存在梯度消失
解决了0均值问题

RELU
训练脆弱

maxout


交叉熵

均方误差
##模型的表示能力和容量

##梯度下降法变式
Momentum
动量： 动量计算，基于前面的梯度，也就是说参数更新不仅仅基于当前的梯度，也基于之前的梯度。

动量变形  Nesterov 计算经过动量更新后的位置的梯度
Adagrad
自适应学习率

RMSprop

Adam(可以看成是RMSprop+Momentum)
##Tensor的拼接
torch.cat()
##处理数据和训练模型的技巧
###数据预处理
中心化

标准化

PCA

白噪声

###权重初始化
全0初始化
随机初始化
稀疏初始化
初始化偏置
批标准化
###防止过拟合
正则化
Dropout
nn.RELU(True)
nn.Conv2d  卷积模块

nn.MaxPool2d
nn.AvgPool2d
nn.BatchNorm2s
LeNet 网络层数很浅，也没激活层      1998年LeCun提出
AlexNet相对于LeNet层数更深，第一次引入激活层RELU,全连接层引入Dropout层防止过拟合 有8层   11*11滤波器    2012年冠军

VGG使用很多小对的滤波器 层叠很多小的滤波器的感受野和一个大的滤波器的感受野是相同的，还能减少参数，同时有更深的网络结构  2014年亚军

GoogleNet(InceptionNet)   22层    2014年冠军

ResNet 2015 ImageNet竞赛冠军 残差模块 152层神经网络 恒等映射    H(x)-x

LSTM      nn.LSTM()
记忆状态C 
GRU将输入门和遗忘门合成更新门

生成对抗网络
生成模型      自动编码器  （编码器生成隐含向量  解码器） 变分自动编码器
生成对抗网络的变式和应用

WGAN
Cycle GAN
#caffe


#keras 

#theano

#mxnet

#fast.ai

服务器 anaconda启动jupyter notebook报错

 Error loading server extension jupyter_nbextensions_configurator
 ModuleNotFoundError: No module named 'jupyter_nbextensions_configurator'



 看内存大小
free -m

free -h

MAP(mean average precision)

张量（名字 维度 类型）



CNN 通过浅层卷积提取图
像的细节特征，如边缘、角点等，深层网络提取语义
特征，如类别等

torch.optim 实现各种优化算法包

保存和加载 
torch.save(model,'.......pth')  结构和参数信息

torch.load('model.pth')

sigmoid是logistic分布函数特殊形式

torchvision.transforms     可以图像增强 转化成Tensor

循环神经网络  基于记忆的模型

判别式模型又可以称为条件概率模型，

典型的判别式模型包括 k 近邻、决策树、卷积神经网
络、支持向量机（SVM）等。 


典型的生成式模型有：朴素贝叶斯和隐马尔
科夫模型等。 

自编码器的应用主要有两个方面，第一是
数据去噪，第二是为进行可视化降维。 



用%automagic on 启用自动魔术命令功能后，可以省略百分号“%’
的输入即可直接运行魔术命令

J  焦点上移一个 ce ll;
K  焦点下移一个 cell 熟悉 im 读者对这 模式应该很习惯；
A  在当前 cell 上面插入 个新的 cell;
B  在当前 cell 的下面插入 个新的 cell;
DD 连续按两次 ，删除当前 cell 。这是另 vimer 喜欢”的功能编辑模式如下。

##numpy知识

np.zeros((3,3))

np.ones((3,3))

np.random.randn()

np.random.randint(10 , 20 , 6)    10到20之间 6个随机数

b = np . random.randint(l , 5 , 20) . reshape(4 , 5)

a= np.arange(6)
In [ 4] : a + 5 ＃数组和标量
Out [ 4] : array ( [ 5 , 6 , 7 , 8 , 9 , 1 0 ] )

np.linspace(-np . pi , np.pi , 200)

需要注意 乘法是对应元素相乘 不是矩阵内积 矩阵内积使用 np.dot（）函 数。

Numpy 提供了 些基本的统计功能，包括求和、求平均值、求方差等

a . sum ()
a.mean()
a . std()
a.min()
a . max()
a . argmin() ＃最小值元素所在的索引
a . argmax() ＃最大值元素所在的索引

In [78) : b 
Out[78) : 
array( [ [3 , 2 , 4 , 2), 
[ 4 , 5 , 1, l), 
[ 4 , 4 , 1 , 4) , 
[3 , 3 , 4, 4) , 
[3 , 2, 4 , 5) , 
[3 , 5 , 2, 5])) 
In [85) : b . sum() 
Out [ 85) : 78 
In [86) : b . sum(axis=O} 
Out [ 8 6 ) : array ( [ 2 0 , 2 1 , 1 6 , 21 ) ) 
In [87) : b . sum(axis=l} 
Out [87): array ( [11 , 11 , 13, 14, 14, 15)) 
In [88) : b.sum(axis=l} . sum() 
Out [88) : 78 
In [94) : b .min(axis=l} 
Out[94) : array([2 , 1 , 1 , 3 , 2 , 2)) 
In [95) : b . argmin (axis=l} 
Out[95) : array([l , 2 , 2 , 0 , 1 , 2)) 
In [96) : b.std(axis=l )


In (37) : a 
Out (37] : 
array ( [ [ 1, 4 , 8 , 10], 
(10 , 9 , 6 , 2] , 
[ 1 , 4 ' 10 ' 5] , 
[ 5 , 7 , 1 , 1] ' 
[ 5 , 2 , 2 , 8 ], 
[ 6 , 10 , 10 , 7]]) 
In [40) : b = np . sort (a , axis=1) ＃按行独立排序，返回一个备份
In[41) : b 
Out[41 ): 
array ( ([ 1 , 4 , 8 , 10) , 
[ 2 , 6 , 9 , 10) , 



[1 , 4 , 5 , 10], 
[1 , 1 , 5 , 7], 
[2 , 2 , 5 , 8] , 
[6 , 7 , 10 , 10])

保存为文本格式的可读性好，但性能较低。也可以直接保存为 Numpy 有的二进
格式    np . save ( ’ a . npy ’, a)

c = np.load('a.npy')

#Pandas知识
Panda 是一个强大的时间序列数据处理工具包
Pandas 最基础的数据结构是 Series   另一个是DataFrame

df = pd . DataFrame(np . random . randn(6 , 4) , columns=list ( ’ ABCD ’ ))

df.shape 
[OUT) : ( 6 , 4 )

df. h ead (3 )

df . tail (2)

df. describe ()
describ （）计算出每列的元素个数、平均值、标准差、最小值、最大
值及几种中位数的值

可以很方便地对数据进行修改，如可以添加 列，列名为 TAGo 
(IN] : df (” TAG ” ]=[” cat”, ” dog”,”cat”,”cat”,”cat ”,”dog” ]

df =pd.read csv (’ data . csv ’, index col=O}

我们还可以使用 DataFrame to_ csv（）函数把数据保存到文件中

#Matplotlib知识


#scikit-learn知识
scikit-learn 是一个开源的 Python 机器学习工具包

from sklearn import datasets 
digits = datasets .load_digits ()

这个过程就是数据清洗，即把采集到的 、不适合用
来做机器学习训练的数据进行预处理，从而转换为适合机器学习的数据。

scikit-le arn 所有的评估模型对象都有 fit（）这个接口，这是用来 练模型的接口。针对
有监督的机器学习（如上面的例子〉，使用 fit(X, ）来进行训练，其中 是标记数据。针
对无监督的机器学习算法，使用自t(X）来进行训练，因为无监督机器学习算法的数据集是
没有标记的，不需要传入y

2.8 妇展学习资源、
1. http://scipy ctures.org ，这是一个按照 CC4.0 协议发布的网站，是 个优秀的 Python
科学计算工具包的教程合集。
2. https://docs.scipy.org/doc/ , numpy scipy 的官方文档。
3. https://en.wikipedia.org/wiki/Random_walk ，随机漫步算法。
4. https://en.wikipedia.org/wiki/S ieve _of_ Eratosthenes 埃拉托斯特尼筛法
5. https://en. wikipedia.org/wiki/Monte _Carlo method 蒙特卡罗方法
6. http://pandas.pydata.org, Pandas 官网
7. http://matplotlib.org, matplotlib 的官方网站 ，包含大量 绘图实例。
8. http://scikit-learn.org/stable/documentation.html, scikit-learn 宫方文档。

在模型选择时，我们使用训练数据集来 练算法参数，用 交叉验证数据集来验证参数。
选择交叉验证数据集的成本 lcv（（）） 最小的多项式来作为数据拟合模型，最后再用测试数据
集来测试选择出来的模型针对测试数据集的准确性

在scikit-leam 里，评估模型性能的算法都在 klean. metrics 里。其中，计算查准率
和召回率的 API 分别为 sklean.metrics precision_ score（）和 sklean .metrics.recall_ score （）



squeeze 函数：从数组的形状中删除单维度条目，即把shape中为1的维度去掉


flatten层

https://blog.csdn.net/program_developer/article/details/80853425


很好用的网络可视化工具，专门用来可视化.prototxt文件，你可以将.prototxt文件中的内容复制到该链接点击打开 http://ethereon.github.io/netscope/#/editor
，然后按下Shift+Enter,就可以看到漂亮的网络结构啦


# 支持向量机：
SVM有三种模型，由简至繁为

当训练数据训练可分时，通过硬间隔最大化，可学习到硬间隔支持向量机，又叫线性可分支持向量机
当训练数据训练近似可分时，通过软间隔最大化，可学习到软间隔支持向量机，又叫线性支持向量机
当训练数据训练不可分时，通过软间隔最大化及核技巧(kernel trick)，可学习到非线性支持向量机


# 梯度提升决策树（Gradient Boosting Decision Tree，GB⁃
DT）是一种迭代决策树算法

xgboost
lightgbm
# 主成分分析

主成分分析法是通过数学降维方法, 从众多原始变量中找出几个综合性强的变量来代替原始变量, 这些综合性强的几个变量彼此间互不相关且能最大程度反映原始变量的信息量[5]。

主成分分析计算步骤如下。

步骤1:对原始数据进行标准化处理并计算相关系数矩阵。

步骤2:计算特征值与特征向量。求出相关系数矩阵的特征值λ (i=1, 2, …, p) , 并使其按大小顺序排列, 即λ1≥λ2≥λ3…≥λp≥0, 然后分别求出对应于特征值的特征向量ei (i=1, 2, …, p) , 其中p为相关系数矩阵的阶数。

步骤3:计算主成分贡献率与累计贡献率。经过筛选分析, 选取累计贡献率达到85%以上的因子作为需水量的主要影响因子即主成分。

步骤4:计算主成分载荷。表示原始数据的协方差矩阵的特征值。

# EM算法
EM 算法是在机器学习和计算机视觉的数据聚类
（Data Clustering）领域常用方法之一，在实际应用中，我们所观察到的数据很多时候是不完全数据，而
传统的优化方法在处理缺失数据情况下的极大似然估计会变得异常复杂，由此学者们提出了 EM 算法

M 算法也称期望最大化算法，是一种从不完全数据中求参数极大似然估计的迭代算法

EM 算法经常被应用于混合模型的参数估计当中，众所周知，正确使用该方法的前提是知道混合模
型中子总体的个数（即混合模型中的分布个数）C   